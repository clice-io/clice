改进 clice 让它真的可用

1. 调整 vscode 插件，增加三个选项, clice 路径，配置文件路径，还有就是以什么模式启动。pipe or socket，默认是 pipe，socket 用于调试。增加 indexCurrent 这个命令的实现。

2. clice

TODO: 继续查找

生成 USR ... 关键点

要分很多情况进行讨论，首先

1. 一般来说，我们总是递归的处理其 DeclContext，然后为它发出即可，但是这里有很多例外需要单独拿出来讨论。

2. 对于模板, 我们应该只记录 index 和 depth，而不应该把名字 encode 进去。考虑如下示例

```cpp
template <typename T>
struct X;

template <typename U>
struct X {};
```

即使模板参数名不同，但是显然这两个模板代表同一个符号。所以实际上只有 index 和 depth 是有意义的。这也和 `TemplateTypeParmType` 和 `TemplateTemplateParmType` 和  的 canonical 形式里面存的信息是一致的。

3. 为了区分主模板和偏特化，我们需要对模板参数列表进行 encode

```cpp
template <typename T, typename U>
struct X {}; // -> equals to X<T, U>

template <typename T>
struct X<T, int> {};
```

这里就牵扯到一个比较麻烦的事情，如何对各种不同的模板参数进行 encode？对于类型来说是比较简单的，简单的进行递归处理就行了。最麻烦的地方在于表达式，如何把表达式 encode 进？不同于 name mangling，它需要对表达式进行特殊的编码，因为链接符号名限定在某些特定的符号中，而我们最终会把 USR 计算成 hash 值，不需要关心具体的文本是啥。所以我们的处理也很简单，就是把表达式的 规范化形式 encode 进去即可。


需要重新实现一下索引的查找逻辑，然后支持 partial 结果

首先第一步的逻辑都是类似的，就是根据 cursor 定位到符号，然后有了 ID 之后再去其它的文件找。

然后呢暂时先不支持 partial，看一下查找效率如何。支持 partial result 和沿着依赖图优先查找都是之后的事情了。

为什么，indexer 的逻辑如此难以编写？？？究竟难在哪里 ... 一一想清楚，明天要写出来

1. 关于逻辑复用，目前使用的这个 StringMap 的方案好吗？并不好，它把 Server 部分逻辑的代码强制要求为每个功能一个函数，然而有很多功能的代码是完全相同的，这样并不方便代码复用。为了复用可能得用模板显式实例化之类的技巧，得不偿失，所以首先就得把这个代码改掉。那就手动一个个匹配吧，其实也没多少字符串，速度上差别肯定不大。

2. 关于文件 Content 的问题。首先可以确定的是从索引读出来的所有 position 信息都是 offset 并且是 UTF8 编码的。这里其实就反应出 LSP 协议一个很严重的问题，采用 column 和 line 而不是文件的 offset。把 offset 转成给定的 column 和 line 在客户端做肯定更简单而且效率更高。既然设计失误那就没办法了。那么问题的关键在于这个解码在哪里做？是`Indexer`还是`Server`？对于未打开的文件直接从磁盘上读文件内容就好了，对于打开的文件呢？则必须要用我们从 LSP 协议那边接受的文本内容了，此时文件还没有保存到磁盘上。而 Indexer 显然是没有这个信息的。所以在 Server 这边做感觉比较好 ... 嗯就是这样。

3. 关于不同代码复用的部分，其实不难发现所有的查找其实逻辑都是一样的，分为两个部分。第一个部分是先根据 pos 找到该符号的 SymbolID 信息，然后在拿着这个 SymbolID 去所有的索引文件里面查找对应的关系。不同的地方在哪里呢？definition，declaration，typeDefinition ... 这五个请求的 resolve 部分和 find 部分是在同一个请求里面完成的。而 Call 和 Type 的则不是，是分两步进行 lazy 完成的。这样的话代码复用的主要思路就是分别定义这个两个函数了。当然还有一些区别，那就是把 Symbol 信息转换为结果的逻辑略有不同。这里怎么做比较好呢？目前的做法是通过一个回调函数，但是关于回调函数是异步还是同步有些纠结。不过既然只是对中间结果的处理方式不同，可以在细分成一步然后处理。具体的步骤就是 resolve -> find -> convert 分成三步就好了。

4. 嗯到这里为止代码逻辑其实已经很清晰了，剩下的主要考虑点就在于逐步返回请求结果了。在理解了上述的流程之后这个也是很好理解的，由于在 find 这一步会查找所有的现存的索引文件，逐步返回结果的原理自然也很简单了。那就是读一个文件返回一次（如果有结果的话）。需要考虑的问题是会不会由于返回的数量太多而导致有 IO 压力？比如有几千个文件，就要发送几千次。这种情况出现的概率应该并不高，因为不会几千个文件里面都有同一个符号，绝大部分文件的搜索结果都是空，所以什么都不用返回。之后可能要考虑沿着 include 图进行搜索，优化搜索情况，这个优化并不困难。然后如果一个符号是内部链接，则搜索范围可以进一步缩小到所有 tu 有关的头文件里面。

5. 还有什么问题呢？最难的问题也是困扰我最久的问题就是多个协程间的状态同步了。考虑如下情况，假设用户已经发送了一个 find reference 的请求，与此同时有更新了一个文件，发送了一个 index 请求。导致索引变化，这种时候该怎么处理比较好呢？首先既然是逐步返回结果，那么已经发送的部分肯定就没法办法了。关键问题在于后续的查找要不要取消呢？Indexer 的状态本身是可能被多个协程更改的，我们应该总是保留最新的状态然后取消之前的吗？

## 协程状态同步 终焉篇

1. 一句话概括模型，调用异步函数返回协程句柄，然后使用 schedule 调度它，然后这个协程的 resume 就会在事件循环里面调用，调用 resume 就切换到下一个挂起点
2. 谁来销毁协程？一般来讲使用 RTTI 来销毁协程就好，Task 是协程的一个 light handle，析构函数里面摧毁协程句柄。但是对于 top level 协程或者在同步函数里面创建的协程这样就不太容易了。比如 libuv 的 read_start 每次读到数据都会调用回调函数。而我们就在这里给每个请求创建一个对应的协程，显然这个 top level 的协程没人存。这当然是我们的问题，注意 LSP 支持请求，所以我们必须有一个全局的 map 保存所有的协程 handle。存是存了，该在哪里销毁呢？一种可能的解决办法是添加一个 flag 用于只是是否在协程执行完毕的时候销毁协程

cancel 基本实现，所以接下来的要处理异步编程中的哪些任务呢？

1. 异步编程的要领，首先要确定，是否有多个异步任务同时修改状态？这个很关键 ... 以 Indexer 为例，要想清楚哪些状态可能被多个协程访问。然后加协程锁 ... 
2. 异步遍历 StringMap 的问题，应该没有解决办法，全部拷贝一遍吧 ... 文件路径倒是好说
3. 有一个关键性的问题 ... 如何处理 CDB 文件更新呢？ ... 例如索引任务正在进行，如何处理 CDB 文件更新 ... 等等 ...，单文件索引和多文件索引同步 ... 想清楚有哪些人协程可能会访问贡献状态是很有益处的 ... 

异步编程的关键是明确协程共享访问，首先以 Indexer 为例，要响应哪些请求？哪些可能会修改状态？谁要读取状态？

1. 首先要响应的最直接的就是 indexAll 命令，也就是索引所有文件
2. 需要响应 CDB 更新，也是就是索引所有编译命令变更了的文件
3. 需要响应头文件更新，重新索引包含了该头文件的文件
4. 需要响应单个文件索引的请求
5. 需要响应查找请求，该请求需要访问共享的全局状态

对于单个文件来说，如果它上面有索引任务，如果有新的任务的话我们总是希望取消掉旧的然后开始新的任务。一但旧的任务被取消了就不用担心它会做任何对全局状态进行修改的地方了，自然不用检查 version。

1. 探究清楚初始化的错误处理，初始化出错了，handle 还需要 close 吗？
2. 把文件系统的请求也合并到 uv 那个基类里面（编写测试记得）
3. uv close 应该是类型安全的
4. uv stream 的 cast 也应该是类型安全的
5. 把网络相关的操作同样封装到协程里面
6. 进一步探究 cancel 的情况，比如 open 一个 file 然后操作被取消了如何撤销？或者干脆用同步的 open，异步 open 也许没用？看看能不能把 uv_cancel 融合到我们的框架里面

思考一下 indexer 可能有哪些状态？

总的来说 indexer 总是以较低优先级对源文件进行编译和索引，索引可能是一个文件也可能是一组文件，有时候还需要有查询索引的状态。

基本想法，为每个文件维护一个 `Task` 这样我们总是能确保旧的任务被取消。

考虑如下场景，我们正在对整个项目的源文件进行 index，但这时候有一个 CDB 更新的消息来了，应该怎么办？

毫无疑问，正确的做法是对整个项目重新进行一次编译。当然由于本来我们就已经有检查是否要重新更新的逻辑了，所以不需要更新并不用太担心。尽管如此，还是希望之后的 CDB 管理器能自己筛选出哪些文件需要更新。而不是在索引器这边处理。

首先仅考虑实现 index 一组文件如何实现？直接使用 gather 即可，它会并发的构建这组文件。

处理如下场景

1. 索引器已经开始了 indexAll 的请求，然后 CDB 更新了，又来了一个 indexAll 的请求。

这时候我们应该怎么做？粗暴做法，直接取消掉旧的请求，然后开始新的请求

但是这样无法处理

1. 索引器已经开始了 indexAll 的请求，后续又要更新一组文件

显然我们不能直接给旧的请求取消掉，那么问题的关键在哪？

首先我们需要根据任务进度来做这个事情，也就是索引器应该有一个东西来表示当前的索引状态，哪些文件是已经完成的，哪些文件是未完成的？

直接把请求加入等待队列不就行了？

从查询的角度来说，indexer 应该支持哪些功能的查询？

1. 查询一个头文件的 header context（）
2. 在 header 和 source 之间切换，这个功能很好做，只需要找到包含该头文件的源文件然后用最匹配的路径即可
3. resolve 一个 position 到符号
4. 给定文件和 symbol 找到该文件中对应的关系，一个 gather 也许能搞定，逐步返回结果之后再优化了
5. 这里有一个问题，对于 index 查询我们总是查询所有可能的索引文件。但是对于 feature index 则需要选择一个上下文来进行了，不同的上下文结果不同。所以这里就需要一个额外的字段记录当前活跃的上下文并且允许用户切换 ...

总之这里有很多的功能，他们全都是基于当前的状态，并不需要任何调度和处理。是在 indexer 里面实现还是 IncludeGraph 呢？ ... 看起来在 Indexer 这边实现比较好，主要是因为 indexer 现在代码太少了，就一个入队啥的。我可不想 IncludeGraph 实现一遍然后在转发一次

好的，现在 header context 的切换搞定了，下面就是两个 Symbol 相关的查询了 ...

这里比较麻烦的地方在哪里？其实主要麻烦的地方就在服务器内部使用的格式都是 offset，但是 Server 那边接受的都是 line 和 column，而进行这样的转换需要文本内容。对于磁盘上的文件，我们直接异步读取就好，麻烦的地方在哪呢？在于打开的文件。打开的文件的修改还没有同步到磁盘上，所以需要通过 LSP 请求进行同步。这里很可能需要一个新的类来执行相关的操作 ... 因为对于 opened file 的处理我之后是专门打算用一个新的类维护的。 ...

好吧，那这里就实现两个 Resolve 和 lookup 函数，而且并不复杂具体的解码。具体的解码交给 Server 那边去做喽 ...

重新思考，如何复用代码 ...

查找请求主要就四种

1. 简单通用查找 ... 流程是什么？先把 position resolve 到一组 symbol。然后在所有的索引文件里面查这组 symbol。然后返回结果
2. Hierarchy resolve 把 position resolve 到 symbol，但是有个问题是什么呢？它需要一个 Range 和 Selection Range 这意味着我们需要找到它的一个声明。由于 Hierarchy 只有两种情况，一种是函数调用一种是类型继承，都要求单独的向前声明/定义可见，所以我们找这个 Tu 关联的索引文件即可。
3. 根据 Hierarchy 的输入读取 SymbolID 并用它查找所有的索引文件 

从流程上来说，主要就是 resolve 和 lookup 两个步骤，前者拿到 SymbolID 后者则在这个基础上进行查找。

第一步需要的结果其实都是严格确定的也就是彻底的 SymbolID , 这个单独抽成一个函数就好。也不涉及到 Position 的转换，很简单 ...

关键在于第二步，第二步毫无疑问就是在一组文件上运行查找，关键点在哪里不同呢？就在于对结果的处理不同 

1. 简单查找，返回所有的 URI 和 Range 即可，URI 在一个文件上是已知的，这个是 when all 
2. Hierarchy 查找，返回第一个找到的 Range 和 DefinitionRange 即可，严格来说是 when any
3. incomingCalls 找 caller，这个必须要查找所有的源文件，是 when all，而且还有一个特性，就是 caller 的 definition 和这个关系在一起。不需要做额外的操作去 resolve  
4. outgoingCalls 找 callee，这个只需要找到第一个源文件就好，第一步是 when any。还有第二部，类似于 Hierarchy 查找，也是 when any 主要是根据把 item 的 range 和 selection range 确认下来。在同一编译单元内部找就行了
5. type hierarchy 的处理和上面那个的步骤基本完全一样，只是对结果，处理不太一样

到这里我们能抽象出哪些操作？

1. 首先我们要有一个函数能确认获取所有索引文件的 path，也要能获取专门和一个 TU 关联的 path。
2. 然后我们要有一个函数用于 resolve position 到 SymbolID
3. 然后我们有 SymbolID 了，也有 Relation 了，该在给定的索引集合里面查找了，加一个变量用于控制是 when all 还是 when any 以适用于不同的场景。得传入一个回调函数用于处理结果。这个回调函数接受一个 content 参数和一个 index::SymbolIndex::Symbol ..，然后 user 可以把它 merge。注意这个函数是在主线程执行的 ...

- 对于查询请求 1，简单的对把 offset 转成 Position 即可。
- 对于查询请求 2，情况是类似的，额外加一个 selection range 
- ... 还有几个请求待会再改，谈谈别的事情



现在我需要更新索引降低 bolt 代码 ...，有如下几个问题需要解决

1. 索引文件是否需要保存？源文件文本？ ... 虽然很不情愿，但是我现在非常希望这样做，保留源文件文本这样可以自解释舒服很多。而且也可以更好的封装函数来优化 content 的合并操作。
2. 把其他的 feature 移植到 feature index 这边来，这个难度并不高，然后封装出相应的函数用于从索引中进行数据读取 ..
3. 重构一下 index 的代码部分，目前暂时把宏 body 体内的高亮关掉了，这个太影响 header context 这个特性了。之后会把它和模板实例化单独做成一个索引文件。可能就是模板实例化附加信息这种？一个实例化一个文件 ... 以后再说了 总之情况是这样 ...

HeaderContext 在宏和模板的处理上并不好，之后在慢慢调整吧，都不是一蹴而就的。尤其是模板实例化，这是一个非常非常非常非常复杂的问题。可以留到后面再说，并不影响现在 header context 的体验。大不了，主要在 C 语言上面实验就是了。


现在 index 有哪些问题需要处理呢？

1. 把 hash 计算移动到 index 这边，让索引器那边干净点 
2. 实现 DocumentLink 其实只有 header 和 has_include 两个，这个很好处理
3. 为 hover 支持索引，然后为 feature index ...
4. 

所以最近是最终决战了，一共有下面这几件事情，记得爆肝解决

1. 把那三个 feature 进行重构，适配到 feature index 里面
2. 实现 DocumentLink
3. 重构 Server 准备好最终的决战
4. 把剩下两个索引请求实现完成
5. 添加 Server 的基本运行测试，确保在一系列 action 下不崩溃

Server 的部分其实主要就是把 capacity 写好，然后转发到对应的函数处理请求，我们这边暂时不太需要处理特别复杂的请求。

怎么处理 header context？刚才写代码给我头写晕了，所以问题出在哪里呢？

## 拯救 clice 作战计划

预计花费一个月解决

1. 首先就要解决的就是 PCH 和 PCM 构建的相关问题，更具体来说有下面这些问题需要解决
    - 实现一个 Command Mangling 的类，用于把不同其它编译器的参数映射到 clang，并且裁剪掉对前端没有帮助的多余的参数
    - 对于 PCH 来说，尝试使用链式 PCH 构建

Scheduler 负责调度所有 AST 相关的构建任务执行，应该包含遍历 AST 的任务吗？
构建链式 PCH，依赖的 PCM，构建 AST，索引源文件 code completion 等等等。
它能够较好的处理 不同任务的优先级，并且支持取消正在尝试构建的任务 ...
值得注意的是 Scheduler 本身不负责任何 AST 状态的保存，具体关于构建状态的保存
交给其它的类，比如 Indexer 之类的。它的职责非常单一，准备构建，并返回运行结果。
不过它确实会维护构建好的 PCH 以及 PCM 的信息。

现在需要思考一下该如何设计接口，首先 CDB 的引用一定是要拿的，由于它接管所有的编译任务倒不如说
只有它才需要访问 CDB。关于编译的接口如何设计和？首先对于普通的源文件编译就很 trivial 了基本啥
都不用干。比较重要的是关于 PCH 和 PCM 的构建。这部分逻辑百分百应该在 Scheduler 内部进行处理，
这是外部无感知的。

Scheduler 的主要接口就是为一个文件构建 AST。

- 为静态索引文件构建 AST

支持递归类型进行二进制序列化 ... 

需要再结构体 T 的 member 中排除掉 vector<T> 不然就会递归编译失败 ...

上面这个已经支持了

接下来的任务？ ... 

1. 测试 SymbolIndex 和 FeatureIndex 的接口 ...
2. 给 DocumentSymbol 的 Feature 添加全面的测试
3. 清理 SourceConverter 的代码（全面清除该函数的使用）
4. 清理 InlayHint 的代码 
    - 确定一些 InlayHint 支持的选项，比如长度限制，但是这里有一个问题，那就是给`auto`提示时候的类型长度，由于不希望配置文件影响索引的行为，似乎只能消极的都存了？代码量可能非常巨大
    - InlayHint 中的符号跳转问题，这个的策略也发生改变了，决定存 SymbolID 然后使用 InlayHint 的 resolve 功能来解决这个问题。
    - 处理好 macro expansion 的挑战 ...
5. 清理 LSPConverter 的代码（现在文件的内容可以直接从索引中读取了，不需要额外的 IO 操作了，当然也可以不是异步函数了）
6. 重构索引器对符号查找的实现，以满足不同的需求。比如有的请求是只需要找到一个就行了，有的则是找到多个。以及假如多个请求同时发生，比如两个查找 declaration 和 reference 的请求被连续发送，是否可以将他们进行一定程度的合并？？ .... 待考虑 ... 还有就是沿着 include 图优化查找了 
7. 给测试文件添加 zip 的功能。

上面这些功能全都写完之后，clice 0.1 宣布公测！

当然这里也还有一些细节上的处理
- 需要对整个服务器中的 assert 或者 abort 进行管理，例如  real path 需要支持错误处理而不是崩溃，因为用户可能会删除文件。
- 某些对 token 序列的处理，比如 InlayHint 或者 FoldingRange 里面需要找括号，由于用户在输入阶段，AST 可能不完整，这些查找算法一定都得设置一个下界来进行处理，防止一直找下去，给服务器卡主了，同样由于 AST 不完整，很多断言并不完全正确


最近修这个 Bug 修的有点烦了，还是先别急慢慢迁移吧。先把 CodeCompletion 的代码稍微写一写？这个和其它的 feature 完全独立的 ...

把 LSPConverter 里面的 PositionConverter 考虑提取出来，忘记测试一下 clang 里面的 column 是 lineOffset 还是 UTF8 字符数了，明天测一下。

目前除了 Server 这边，所有的测试代码都在迁移到使用 offset 表示，而不是 line 和 column，但是很尴尬的是 CodeCompletion 使用的是 line 和 column，为了测试方便，考虑把函数提取出来。但是注意 proto::Position 仍然不是我们能稽越的。如何处理呢？这是一个值得思考的地方。注意这里仅在测试用例中使用，所以还是把 PositionConverter 暴露出来然后重命名吧。

然后可以开始测试 CodeCompletion 的实现了。

除此之外测试 SelectionTree 的实现也是不错的。当然还有终极目标重构

算了，还是先不搞 CodeCompletion 了，

优先重构 InlayHints 然后把 SourceConverter 清理了（顺便清理一下文件路径）。

最最优先级的任务，考虑这个周末内实现。

SourceConverter 已经彻底清理了，这真是一个好消息。

现在我的想法又变了，想要先把 Scheduler 实现了，具体怎么做呢？感觉有很多值得考虑的地方哦。

首先思考一下 Scheduler 的职责，语言服务器可能会有很多 CPU 密集型的任务需要执行，把它们都在线程池中调度执行

具体来说目前最基础的目标是实现一个编译任务

也就是在用户实时编辑文件的时候能即时的构建 AST，这个听起来是不是很简单，


具体要实现何种效果呢？这里把 PCM 支持也顺便做了

1. 输入，用户打开一个文件，我们要返回一个构建好的 AST，检查一下是否有正在等待或者构建的 AST，如果有取消这个任务

2. 判断这个文件是不是 module unit，不是直接到构建 PCH
3. 如果是 module，我们需要递归检查它依赖的文件是否需要更新，把直接获取到这个 module 直接和间接依赖的所有 module（拓扑排序依赖图）。换句话说，我们需要检查这些所有的 PCM 是否需要 rebuild。同样也要 check 它们的状态，等待 rebuild，正在 rebuild 或者已经 build 好了。
4. 判断方式就是判断它依赖的源文件和头文件 mtime 是否大于构建它时候的了。


我发现任务的状态都有三种，running，pending，和 idle。当一个新任务到来的时候，我们一般首先看有没有和这个任务相同的正在执行的任务。如果有的话视情况需要取消。如果正在 pending 就把任务信息更换成新的，如果没有就开启一个新任务。

所以问题的关键在于什么？就是如何找到我们储存的状态

以 PCH 